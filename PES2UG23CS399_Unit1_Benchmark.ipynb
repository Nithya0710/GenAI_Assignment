{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ad6f45",
   "metadata": {},
   "source": [
    "# Unit-1 Assignment\n",
    "\n",
    "### Name: Nithya Sri P. B.\n",
    "### SRN: PES2UG23CS399\n",
    "### SECTION: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a5c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nithya/Desktop/College/GenAI/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633c45e",
   "metadata": {},
   "source": [
    "#### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model=\"bert-base-uncased\"\n",
    "roberta_model=\"roberta-base\"\n",
    "bart_model=\"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20f65f",
   "metadata": {},
   "source": [
    "#### Experiment 1: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b251f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1699.02it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Output:\n",
      " [{'generated_text': 'The future of Artificial Intelligence is. as as the and new / or or and - -, ( \" \" \". the many the the or with many so \" ( ( \". the and and and and and \" ( \" ( ) ( \". him for \". it it it it it that and and and and - ( \" ( \" ( \" ( ( \" ( \"., \". the an an an s and the more much much much much much much much much many so and the the and the the in the and and and the the so jeffersonyt \" ( - - - - - - - ( \" ( \" ( ) ( ( ( ( ( ( - - ) ( ( ) ) ( ) ( or - - \". it it it was hailey who with \". a ( ) (, ( and \" the bar as ari to time as more actually and on the in so jefferson tis how it especially for\\'\\' - - -, ( (. it to his \" (.. more found that. it on it it or so the the feel were hands for \" king was \" ( ) as before band are so \" be so and and and \" \". a under by \". it more actually or some some some some some some some people people.. it more'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1606.14it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Output:\n",
      " [{'generated_text': 'The future of Artificial Intelligence is'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 159/159 [00:00<00:00, 1593.54it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]   \n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Output:\n",
      " [{'generated_text': 'The future of Artificial Intelligence is pertaining erect Features Sie infection Features Features Features infection Featuresist Features infection websites infectionlb Features infection render Features Features Superintendent infectionRs Features FeaturesAS Features infection infection Holmes infection infection infection consulting infection Features hand infection Features infection War War War infection infection Canterbury infection unch infection infection unse infection infection War infection Features War infection expelled infection War blackmail Features FeaturesGs infection Featuresimar infection infection angry infection Warond Features blackmail infection infection FeaturesSend Features infection picture infection infection Chad infection infectionond1973 infection infection Iron infection infection blackmail infection Cycl War infectionimar infectionopolis blackmail infection src Features infectionopolis War blackmail infection Features meant infection infection offensively infectionbis infection Warbis infection infection hailed infection infectionbisbis infection hours infection infectionopolis infection infectionSend infection Warimar War Features infectionbis Features infectionimaropolis angry infection infection forecasting infection infectionms infection infection expelledbisuclearbis infectionimarbis infectionbis blackmail Tart infection infectionAA infection blackmail blackmail blackmail infection blackmail hailed infectionbis Hebdo blackmailbis Features blackmailbis infection Features blackmail blackmail Features blackmail Lenovo blackmailbis blackmail blackmail hailedbisbisbis Features FeaturesWindow spring blackmail Features giganticbis Featuresbis hailed gigantic blackmail blackmail kings giganticbisrab Features blackmail War blackmail blackmailimar Features regulators blackmail blackmail Rus meant blackmail blackmail apost meant blackmail Features angry Features Shape ke gigantic gigantic ke Features hailed blackmail Pet Features Features blackmail gigantic blackmail gigantic'}]\n"
     ]
    }
   ],
   "source": [
    "prompt=\"The future of Artificial Intelligence is\"\n",
    "\n",
    "models={\n",
    "    \"BERT\": bert_model,\n",
    "    \"RoBERTa\": roberta_model,\n",
    "    \"BART\": bart_model\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        generator=pipeline(\"text-generation\", model=model)\n",
    "        output=generator(prompt, max_length=30)\n",
    "        print(f\"\\n{name} Output:\\n\", output)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name} Failed:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0495",
   "metadata": {},
   "source": [
    "#### Experiment 2: Masked Language Modeling (Fill-Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1689.17it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Predictions:\n",
      "create - 0.5397014021873474\n",
      "generate - 0.15575793385505676\n",
      "produce - 0.054055824875831604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 1710.80it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Failed:\n",
      " No mask_token (<mask>) found on the input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 1809.44it/s, Materializing param=model.shared.weight]                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Predictions:\n",
      " create - 0.07461672276258469\n",
      " help - 0.06572078913450241\n",
      " provide - 0.060881923884153366\n"
     ]
    }
   ],
   "source": [
    "sentence_bert=\"The goal of Generative AI is to [MASK] new content.\"\n",
    "sentence_bart=\"The goal of Generative AI is to <mask> new content.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        fill=pipeline(\"fill-mask\", model=model)\n",
    "        sent=sentence_bart if name == \"BART\" else sentence_bert\n",
    "        output=fill(sent)\n",
    "        print(f\"\\n{name} Predictions:\")\n",
    "        for o in output[:3]:\n",
    "            print(o[\"token_str\"], \"-\", o[\"score\"])\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name} Failed:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47443d0f",
   "metadata": {},
   "source": [
    "#### Experiment 3: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72342e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1152.09it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]             \n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Answer:\n",
      " {'score': 0.008749512024223804, 'start': 32, 'end': 81, 'answer': 'risks such as hallucinations, bias, and deepfakes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 197/197 [00:00<00:00, 1700.46it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Answer:\n",
      " {'score': 0.013442824594676495, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 259/259 [00:00<00:00, 1717.83it/s, Materializing param=model.shared.weight]                                  \n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BART Answer:\n",
      " {'score': 0.057058071717619896, 'start': 38, 'end': 81, 'answer': 'such as hallucinations, bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question=\"What are the risks?\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        qa=pipeline(\"question-answering\", model=model)\n",
    "        output=qa(question=question, context=context)\n",
    "        print(f\"\\n{name} Answer:\\n\", output)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name} Failed:\\n\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
